
# Introduction

La réelle limite des bases de données traditionnelles est sans aucun doute la gestion de très grosses quantités de données. À titre d’exemple, certains analystes n’hésitent pas à affirmer qu’environ 2,5 trillions d’octets de données sont produits par jour (années 2000). C’est considérable et ce type de chiffre ne cesse de fluctuer à la hausse. Au cours des années 2000, des entreprises telles que Facebook ou Google ont dû se confronter à ce problème de Big Data afin de pouvoir proposer plus de services avec davantage de données à gérer et de variété à prendre en en charge. Il a alors fallu penser à d’autres modes de gestion de données et proposer une alternative à nos fameuses bases de données relationnelles (SGBD-R).
# Qu'est ce que le Big Data ?

Le terme **Big data** à un ensemble de concepts, d'outils ([[Les outils du Big Data]]), de techniques et d'usages liés à l'exploitation intensive des grandes masses de données.

Le big data se défini à son origine par 3 caractéristiques. Les 3V puis c'est décliné en 6V. Le concept a été initié par Doug Laney en 2001.

**Volume :** Cela peut paraître évident quand on parle de Big Data mais bien sûr la qualité première d’un système Big Data est d’être capable de gérer une très grande quantité de données. Cette quantité de données stockées pourra même (très vraisemblablement) ne cesser de croître au fil des jours/heures/secondes, etc. À l’heure actuelle, on estime la quantité de données dans le monde à plusieurs dizaines de zettaoctets, soit plusieurs milliards de téraoctets ! Et bien sûr ce n’est que le début...

**Variété :**  On ne se limite plus aux données structurées. Il devient important d’ingérer toute typologie d’information: structurée ou non ! Ces données, malheureusement sont souvent difficilement exploitables en tant que telle, par ailleurs et au vu de la volumétrie, il est souvent impossible de les analyser manuellement.

**Vélocité :**  Les systèmes utilisé doit pouvoir ingérer et mettre à jour en temps réel ces informations pour qu’elles puissent être analysées rapidement. On parle de **vitesse d'acquisition et de transmission**

*Si les 3V sont en quelque sorte à l’origine du cahier des charges de cette nouvelle manière de gérer les données volumineuses (Big Data), on peut voir se rajouter d’autres V qui enrichissent et permettent de mieux définir de nouvelles exigences fondamentales.*

**Véracité :** Cette exigence indique clairement que les données doivent être de bonne qualité et de confiance.

**Valeur :** Les données stockées doivent avoir un intérêt et donc apporter une réelle valeur. Face à la boulimie grandissante des Systèmes Big Data, il n’est plus question de stocker pour stocker ! La valeur de la donnée résulte de ce que l'on arrive à tirer de son exploitation en terme de valeur économique et statistique.

**Visualisation :** les données doivent être mises à disposition pour être visualisables.


#DATA #BIGDATA

Autres lectures :

